{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Mk8w7nDJfX"
   },
   "source": [
    "# Fine-Tuning Mistral 7b v0.3 Model for Classification of Phishing Emails Using Unsloth Techniques\n",
    "\n",
    "###Overview:\n",
    "\n",
    "This project focuses on fine-tuning the Mistral 7b v0.3 model using Unsloth techniques for the classification of phishing emails. Beginning with the utilization of Unsloth for efficient fine-tuning, the project aims to optimize the training process by leveraging low-resource techniques, thereby reducing training times and improving resource efficiency.</br></br>\n",
    "\n",
    "The next phase involves utilizing a phishing email dataset to train the model. This dataset provides labeled examples of email content categorized as safe or malicious, essential for training the model to accurately classify unseen emails based on their textual content and other relevant features.</br></br>\n",
    "\n",
    "The SFT (Self-Supervised Fine-Tuning) Trainer is employed to facilitate effective model training. This includes configuring training parameters such as batch size, learning rate, and optimizer settings to maximize model performance and accuracy in distinguishing phishing emails from legitimate ones.</br></br>\n",
    "\n",
    "Finally, the project demonstrates the model's inference capabilities for email classification. Users can input email text, and the model predicts whether the email is safe or potentially malicious in real-time, showcasing the practical application of the fine-tuned Mistral 7b v0.3 model in enhancing email security through automated classification.</br></br>\n",
    "\n",
    "This comprehensive approach highlights the integration of advanced natural language processing techniques with practical cybersecurity applications, aiming to improve detection and response to phishing attacks using state-of-the-art language models.</br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72yki1yTKLpc"
   },
   "source": [
    "#Setup and Installation :\n",
    "\n",
    "1. Imports torch and gets the CUDA device capability.\n",
    "2. Installs the unsloth library from GitHub, suppressing the output to keep the notebook clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpILfWPqJcoS"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncUQbMF3l8lp"
   },
   "source": [
    "Note : There are different methods to download unsloth for different GPUs. The following process is specific to the T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94cUdV1ZLTp2",
    "outputId": "472945ca-ca66-4324-d477-8cef5415a5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers\n",
      "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trl\n",
      "  Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, xformers, trl, peft, accelerate\n",
      "Successfully installed accelerate-0.32.1 bitsandbytes-0.43.1 peft-0.11.1 trl-0.9.4 xformers-0.0.26.post1\n"
     ]
    }
   ],
   "source": [
    "# Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5SD-e3LMbE9"
   },
   "source": [
    "**Initialize FastLanguageModel**\n",
    "\n",
    "*   Imports the FastLanguageModel from the unsloth library.\n",
    "*   Sets the maximum sequence length for the model.\n",
    "*   Configures the data type for computation based on the GPU type.\n",
    "*   Enables 4-bit quantization to reduce memory usage, which can be disabled if not needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N0az3B0LUcW",
    "outputId": "ac2dcdc0-3040-44fe-a6ea-6e6267de0216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# dtype = torch.float16\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EfWjKCK1UoM"
   },
   "source": [
    "**Loading the model**\n",
    "\n",
    "This cell loads the Mistral AI model with 7 billion tokens, version 3, from the unsloth repository. It also defines the model and tokenizer in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345,
     "referenced_widgets": [
      "3a459bb580f4450186120a4b8f57942e",
      "7dc019e12d864ef282c67d252409268b",
      "158307d372c045369f2b5846b10ba709",
      "8d54540c69ae4eaf8ae7bcadcced05eb",
      "dd981707e2d54805b575b16e26c1f208",
      "51a63487a9eb491890df871379a43d84",
      "a4a6f790029848f180498247548e3312",
      "babe7affccd04fe19cbf1c43eae31047",
      "95a7a379600e4261942daa4ee7c5f841",
      "f0359462ad9c44349852d9b6872cad5c",
      "d4da84359f924e209dce19f5fa3c7545",
      "184fa8c9699a4fb3a40a13114eae2f60",
      "060aab2497454d9f88ada7f527e7c527",
      "90a388c774a548a0a6a95d296cc950cb",
      "437b943f1407428e92fbb3c7bd0f3aa6",
      "4e52bee119ae453c9e938416ce324167",
      "eed588b1bbe84d97b80746dc0c18cfc9",
      "790191f7eda3481bae622d48a2b33f08",
      "dcc10006a1db48b58b87eb3ac1efa58a",
      "80d3e12854ba41ceb5274bb22b87931e",
      "a5e8e712f05a4d2097aca52b1e343344",
      "95191e350b20421da9a865a12e87331d",
      "4b580038b2814133b113e55cb664b3fc",
      "2ad6d1d8aaf24f79b42fd6a04c876aef",
      "e48513d79f37446190772640b49a1beb",
      "56ef9f2beda0451cb0dacf7e3d94a170",
      "bc6ecd6f94a440f99f995c7b0a6923b5",
      "0e67c5505711436ab0f0552bbf79d31c",
      "bb750dcc126b4a9ca0920aa47b5c835c",
      "a931ec9d4a5141d0bf3e2945cea375d6",
      "5d6f3fe422c14f72b8722352d93fa1f4",
      "999eba1286cd4b1d8241d67ac9f1f487",
      "8c6327bfa97340b391f2c7b810f018fe",
      "eb28e0440d9e4ef593c8fef50c20e57b",
      "45886fbcf721429a9714b1daee093e85",
      "2865925746314c04888abf73ecb5d442",
      "b1ef5e2477bc475b91ed185e7ce29e78",
      "2b6358b746014c6485f696413e9cdb02",
      "3e753dcbd3c74149ba22361bef6d1533",
      "fbbc7aeefbf34dc0bb6c1a6f5a65114a",
      "19347f22ea16407abc4a70c5358b9899",
      "ba72b0fce9d741e5babe06bc0618a63a",
      "7dee278521b5431893dde42d0deb688e",
      "689ff3b667724852ba8558726fbb7ab2",
      "ef61ce1a320a4944bc0b041a97b3ce03",
      "b4b98d5c60004c72ac33fd328c7a9798",
      "dbd0666b8b3542b6ae4563b273516f2c",
      "9d16ffebd7a946a5bad7b4be23b47b2d",
      "a1d69f7c9e5944d4807b54184eeecb73",
      "ebfa8bd7f9f64334904e40f4b5ca1c2e",
      "082e5ec2a39b498c91ced598e61c783f",
      "cfff882793ad44febba00f2fee92a4b1",
      "dadd91766f044fd48366034dcbf8d5cb",
      "5ec81493e2854e149cfa47d71a6e33f8",
      "dbbe0f7d8db54ad2a617f3f742872791",
      "15be1830e8084e9eb2d757f321bc32fe",
      "b848dfd1448545e09f236deae96de482",
      "62d0258cd5694fc397eec39a7fef263a",
      "a0e90b452f3f41f69c2ce281b018d531",
      "219f1e62ab3947f19776a7b35d428778",
      "9969c1d147bd4592b8c12b5376544f7e",
      "e4dc067a2f064cd1815098314cedb809",
      "1ebae4bb93994bd68e9e0d9a9886d90d",
      "d32ee3802cc344dca67e7e39a47e90a5",
      "1259b966e3aa4da49c9b80637077f668",
      "86972579cd8142479f14bcede4c7fd9c",
      "1888356dbc944862aa570de1e1d43fc4",
      "98945d2420cf4230bfdfea8ce0c7732e",
      "72f3848aa42b488394fbbcf9ef263a0f",
      "fbc26c48e51541379ddaff4f41c7cc08",
      "c4ca05089b1941369d5e866b3fc4aefa",
      "a3fd1d4f67ec4866a952c9395fa99541",
      "25d5c31a3bfd4985b7180a6f316d696b",
      "b55057927caf492faab07ccd604fc5fb",
      "32d256cbc70144e381bebeb7359e98c9",
      "172656270dc8443184516dd3eeace333",
      "f22307f89292492aa2799467e48fc60b"
     ]
    },
    "id": "ax05fSQZXxt2",
    "outputId": "8dce2ac6-daad-4400-bc3b-5b087e32827c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a459bb580f4450186120a4b8f57942e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.7\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184fa8c9699a4fb3a40a13114eae2f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b580038b2814133b113e55cb664b3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb28e0440d9e4ef593c8fef50c20e57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef61ce1a320a4944bc0b041a97b3ce03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15be1830e8084e9eb2d757f321bc32fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1888356dbc944862aa570de1e1d43fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load unsloth/mistral-7b-v0.3-bnb-4bit as a legacy tokenizer.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l19tDy0t3Yae"
   },
   "source": [
    "### Configure Model\n",
    "**Note**: We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "* Model: Uses the previously loaded model.\n",
    "* r: Rank for low-rank adaptation (e.g., 16).\n",
    "* Target Modules: Specifies the modules to apply PEFT.\n",
    "* LoRA Alpha: Scaling factor for LoRA.\n",
    "* LoRA Dropout: Dropout rate for LoRA (optimized at 0).\n",
    "* Bias: Bias configuration (optimized at \"none\").\n",
    "* Gradient Checkpointing: Uses \"unsloth\" for efficient memory usage.\n",
    "* Random State: Sets a seed for reproducibility.\n",
    "* Use rslora: Option to use rank stabilized LoRA (disabled here).\n",
    "* LoftQ Config: Option to use LoftQ (not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DngZCvjwXxzu",
    "outputId": "ee289e7f-0b98-4cb8-e580-8843f91c4d14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # max_seq_length = max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VShjrwThXzLk"
   },
   "source": [
    "# Loading the dataset and preprocessing.\n",
    "This cell loads the Phishing Email dataset from the Hugging Face repository zefang-liu/phishing-email-dataset. The dataset contains two columns: Email text and email type. The cell also includes preprocessing steps to prepare the data for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "c9d434c263df411f960d43acfb832436",
      "9428c81aa203407c9ef1dc5936bccf94",
      "734136c89e3646e28601a5ddc10eb1c3",
      "871791be7ceb438092b5d4678d74420c",
      "6be72000fb4341c58ae329ac38086faf",
      "4f29d5902dd2454ba34a8383fd67d175",
      "3a1d5524de104dd795095f52bce7f99f",
      "ad13486a88b54e83a6414176fd870926",
      "de73a587370a42c995cb102b9f43dc54",
      "53a594a3556a4307b2889bde7eb5853e",
      "c70284f31e294611b0fce39ecb22ca22",
      "8af06aa29c44430eb8c48eff8cba8350",
      "bce39ae9d22a4b70a065e409d18d1173",
      "93e305a98a9b412eb848cca5c8e920bb",
      "1cba153024d1488b9318607c5ebabfb2",
      "e748e6356a72487694a0799fc80a47a0",
      "28d5033a850f4de38b441f2dc64d8251",
      "39ab8c90c55440d4b90838f5fa335369",
      "af90d198a92c40aa8bbb3ab9b7c9663f",
      "5d4fb5163dce4b2e8593403d53ce530e",
      "7df2d86b50da49a4b70b89375975167b",
      "b776ad0bc535441db446a7e9e93748a1",
      "0a96d4c901524af8905729fb9a1d4105",
      "b7e07bb4ddc848bfa5c3517040b21c38",
      "e66ab6c6cc394348908654e73fa07561",
      "f71a8225948447f69391598febe172cf",
      "f150520390434a03a3c12a6cf76cd589",
      "ad67e5594d8f44f7af970949af8f2466",
      "307fb91129e34810b4d38eb2d0974cbd",
      "55761d7a97a24e0a91c41c3d088c32aa",
      "78c1849832ca4d18aeb557c766116ac6",
      "588310f536cb4beaa037bf075323738f",
      "021d9934efbc484890aa11514baf6b0b"
     ]
    },
    "id": "cDB5s5e7X6AY",
    "outputId": "8b6a7554-7436-4f4d-e4b7-5e41e30221cd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d434c263df411f960d43acfb832436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af06aa29c44430eb8c48eff8cba8350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/52.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a96d4c901524af8905729fb9a1d4105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/18650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"zefang-liu/phishing-email-dataset\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhLggvl8aVxQ",
    "outputId": "0d95aef0-ed18-43ed-fdeb-909e47ba5de2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'Email Text', 'Email Type'],\n",
       "    num_rows: 18650\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kRuoMKAaN4o",
    "outputId": "328f68bd-9b46-4f0a-eae9-ba68b83dc1f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Email Text', 'Email Type'],\n",
       "    num_rows: 18650\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing unncessory columns\n",
    "dataset = dataset.remove_columns(['Unnamed: 0'])\n",
    "dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2oyJfmsYTKI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGXGQJVjYW6u"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EuREsl4Z5hi",
    "outputId": "8f33ce12-2935-4d10-ce02-dbdd67588404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Email Text      Email Type\n",
      "0  re : 6 . 1100 , disc : uniformitarianism , re ...      Safe Email\n",
      "1  the other side of * galicismos * * galicismo *...      Safe Email\n",
      "2  re : equistar deal tickets are you still avail...      Safe Email\n",
      "3  \\nHello I am your hot lil horny toy.\\n    I am...  Phishing Email\n",
      "4  software at incredibly low prices ( 86 % lower...  Phishing Email\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrOCs3t366BC"
   },
   "source": [
    "## Format Phishing Email Dataset for the training\n",
    "* Defining the prompt template\n",
    "* Adding the EOS token to stop infinite generations.\n",
    "* For training the model, we need all attributes in a single column. We have mapped the email text and email type into a text column using the prompt template and formatting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "03e6e2c9583a4a70be4f62f1d1869f02",
      "e9a535688b604bad8d24c32ab7e0eb19",
      "731e77ce3bed45148d8143ca35700e23",
      "e8176c2635154d68baabb42fff62e7b2",
      "11d08ee59ee34fe6ab64d9fdb9b88fbe",
      "cdb13e99f37849be81fb4351ca6af689",
      "d2c0230538e6437f9120a71193163e33",
      "aae381a64af04601be11f169a6b5e488",
      "97fdbffc4d944a1c99f4a7e3c2a754aa",
      "a8a2f7dcb7614f46a5e769a69d7551f0",
      "11bd52cbb0804ca89c19b2b80556a37b"
     ]
    },
    "id": "d3WBkMpQZ7Hf",
    "outputId": "f56ad576-4d65-4376-9485-edea1818186d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e6e2c9583a4a70be4f62f1d1869f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "phishing_prompt = \"\"\"Below is an email body. Determine if the email is safe or phishing.\n",
    "\n",
    "### Email Text:\n",
    "{}\n",
    "\n",
    "### Email Type:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "# Define the formatting function\n",
    "def formatting_prompts_func(examples):\n",
    "    email_texts = examples[\"Email Text\"]\n",
    "    email_types = examples[\"Email Type\"]\n",
    "    texts = []\n",
    "    for email_text, email_type in zip(email_texts, email_types):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = phishing_prompt.format(email_text, email_type) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGnjjuyN-agR"
   },
   "source": [
    "## Let's Train the Model\n",
    "**Initialize SFTTrainer for Model Training**\n",
    "* Model and Tokenizer: Uses model and tokenizer from unsloth.\n",
    "* Training Dataset: Specifies dataset with formatted text in the \"text\" field.\n",
    "* Training Settings: Configures batch size, gradient accumulation, learning rate, optimizer settings, and other training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "75599a7f2be744178aee51ea9583cdb8",
      "da3beac3adfb47f3b5ed8287911ca3e0",
      "a586813e7081409faaaab7e0ef5d2e86",
      "0821804633374c7bb4834d99e6b5ab14",
      "d72998dbf6a548c1be169a5286881703",
      "b2d6202c09614a60b79c8ed44ad25b95",
      "1504b14b3e0d411694a650a197bf77cf",
      "60dc20a668ad41809bdf2ce1a6867d6c",
      "41f40e575ec84b86980958214a4c7e9b",
      "07a50f33cffd47f6a3cf71566d37ac59",
      "5c209fa652944cd792826cecc52573de"
     ]
    },
    "id": "OPlYPl72ufFA",
    "outputId": "e5a6508e-29b6-451e-be2d-8cf70bc2dd48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75599a7f2be744178aee51ea9583cdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/18650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "an_oJ62pj2j1",
    "outputId": "bdefd0b8-cb54-4e8d-dc10-037ca518d861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.748 GB.\n",
      "4.637 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScaAzDMr_A-d"
   },
   "source": [
    "**Execute Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-7gHfCDykK-F",
    "outputId": "ed08d739-a162-4ba9-fdbf-9cbc64e93d3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 18,650 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 16:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.386300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.920200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.762700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.506700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.687900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.800800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.889700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.784100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z49D48eDoa6G"
   },
   "source": [
    "Final Memory State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hStytXE5kWHd",
    "outputId": "ae5b179f-96f9-49b5-c677-e6ae774642e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017.9514 seconds used for training.\n",
      "16.97 minutes used for training.\n",
      "Peak reserved memory = 6.973 GB.\n",
      "Peak reserved memory for training = 2.336 GB.\n",
      "Peak reserved memory % of max memory = 47.281 %.\n",
      "Peak reserved memory for training % of max memory = 15.839 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlNsXPf1_TL_"
   },
   "source": [
    "## Inference\n",
    "Let's run the model! </br>\n",
    "Give the mail body in the input box and it will show the Mail type whether it Safe mail or Phishing Mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "682hwgkcoeOt",
    "outputId": "afd07170-f7d9-41e2-e891-8ffb041431bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter mail body to Check if it is phishing or not : Dear Customer,  Please find attached your monthly bank statement for June 2024. If you have any questions, please contact our support team.  Thank you for banking with us.  Best regards, Your Bank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>Below is an email body. Determine if the email is safe or phishing.\\n\\n### Email Text:\\nDear Customer,  Please find attached your monthly bank statement for June 2024. If you have any questions, please contact our support team.  Thank you for banking with us.  Best regards, Your Bank\\n\\n### Email Type:\\nSafe Email</s>']\n",
      "<s>Below is an email body. Determine if the email is safe or phishing.\n",
      "\n",
      "### Email Text:\n",
      "Dear Customer,  Please find attached your monthly bank statement for June 2024. If you have any questions, please contact our support team.  Thank you for banking with us.  Best regards, Your Bank\n",
      "\n",
      "### Email Type:\n",
      "Safe Email</s>\n"
     ]
    }
   ],
   "source": [
    "# Define the phishing email prompt\n",
    "phishing_prompt = \"\"\"Below is an email body. Determine if the email is safe or phishing.\n",
    "\n",
    "### Email Text:\n",
    "{}\n",
    "\n",
    "### Email Type:\n",
    "{}\"\"\"\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Example email text to classify\n",
    "email_text_example = input(\"Please Enter mail body to Check if it is phishing or not : \")\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    phishing_prompt.format(\n",
    "        email_text_example,  # email text\n",
    "        \"\"  # email type - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate the outputs using the model\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "\n",
    "# For streaming inference\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr_zriDOqMhI"
   },
   "source": [
    "#Saving the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66_xO7ycpiAp",
    "outputId": "21672713-03c1-47d9-cc51-536706e2d383"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mistral_v3_phishing/tokenizer_config.json',\n",
       " 'mistral_v3_phishing/special_tokens_map.json',\n",
       " 'mistral_v3_phishing/tokenizer.model',\n",
       " 'mistral_v3_phishing/added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"mistral_v3_phishing\") # Local saving\n",
    "tokenizer.save_pretrained(\"mistral_v3_phishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRdhJ5Erx6gQ"
   },
   "source": [
    "**Use saved model to generate inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ema4AordqWuK"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  from unsloth import FastLanguageModel\n",
    "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "      model_name = \"mistral_v3_phishing\", #model which i have saved.\n",
    "      max_seq_length = max_seq_length,\n",
    "      dtype = dtype,\n",
    "      load_in_4bit = load_in_4bit,\n",
    "  )\n",
    "\n",
    "  FastLanguageModel.for_inference(model) #Enable 2x faster inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAX31RlYDHqx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHMGXR-Y0zL8"
   },
   "outputs": [],
   "source": [
    "#lets define the phishing prompt again\n",
    "\n",
    "phishing_prompt = \"\"\" Below is an email body. Determine if the email is safe or phishing.\n",
    "\n",
    "### Email Text:\n",
    "{}\n",
    "\n",
    "### Email Type:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dCdt8n51-iY",
    "outputId": "60e58dce-59e3-4ff4-cd10-cd0a38912035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Email body to classify whether it is safe or phishing: Dear Valued Customer,  Thank you for your recent purchase with us. Your invoice is attached to this email. If you have any questions about your order, please contact our support team.  [Download Invoice](http://phishing-link.com)  We appreciate your business.  Best regards, Customer Support Team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Below is an email body. Determine if the email is safe or phishing.\\n\\n### Email Text:\\nDear Valued Customer,  Thank you for your recent purchase with us. Your invoice is attached to this email. If you have any questions about your order, please contact our support team.  [Download Invoice](http://phishing-link.com)  We appreciate your business.  Best regards, Customer Support Team\\n\\n### Email Type:\\nPhishing Email</s>']\n",
      "<s> Below is an email body. Determine if the email is safe or phishing.\n",
      "\n",
      "### Email Text:\n",
      "Dear Valued Customer,  Thank you for your recent purchase with us. Your invoice is attached to this email. If you have any questions about your order, please contact our support team.  [Download Invoice](http://phishing-link.com)  We appreciate your business.  Best regards, Customer Support Team\n",
      "\n",
      "### Email Type:\n",
      "Phishing Email</s>\n"
     ]
    }
   ],
   "source": [
    "#Take the input of Example email to classify whether it is safe or phshing\n",
    "email_text_example = input(\"Enter the Email body to classify whether it is safe or phishing: \")\n",
    "\n",
    "#lets prepare the input for the model\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        phishing_prompt.format(\n",
    "            email_text_example,  #email text\n",
    "            \"\" #email_type  leave this blank for generation.\n",
    "        )\n",
    "    ], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "#generate the output using the model\n",
    "outputs = model.generate(**inputs,max_new_tokens=64,use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "\n",
    "\n",
    "# For streaming inference\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCw_t73O2Dk0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
